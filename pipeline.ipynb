{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle  # Ajout pour la sérialisation\n",
    "\n",
    "def load_csv(file_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file_path).sample(frac=0.1)\n",
    "    df['Dates'] = pd.to_datetime(df['Dates']).dt.to_period('M')\n",
    "    return df\n",
    "\n",
    "def create_pipeline():\n",
    "    numerical_features = ['X', 'Y']\n",
    "    categorical_features = ['Dates']\n",
    "\n",
    "    numerical_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def train_pipeline(pipeline, df):\n",
    "    X = df[['Dates', 'X', 'Y']]\n",
    "    y = df['Category']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Entraîner la pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Évaluer le modèle\n",
    "    score = pipeline.score(X_test, y_test)\n",
    "\n",
    "    # Sérialiser la pipeline\n",
    "    with open('pipeline_model.pkl', 'wb') as file:\n",
    "        pickle.dump(pipeline, file)\n",
    "    print(\"Pipeline sauvegardée dans 'pipeline_model.pkl'.\")\n",
    "\n",
    "    return pipeline, score\n",
    "\n",
    "def make_prediction(pipeline, input_data: dict) -> dict:\n",
    "    input_df = pd.DataFrame([input_data])\n",
    "\n",
    "    prediction = pipeline.predict(input_df)\n",
    "    return {\"prediction\": prediction[0]}\n",
    "\n",
    "def data_pipeline(csv_path: str, input_data: dict):\n",
    "    # Charger les données\n",
    "    df = load_csv(csv_path)\n",
    "\n",
    "    # Créer la pipeline\n",
    "    pipeline = create_pipeline()\n",
    "\n",
    "    # Entraîner la pipeline\n",
    "    trained_pipeline, score = train_pipeline(pipeline, df)\n",
    "\n",
    "    # Faire une prédiction\n",
    "    prediction = make_prediction(trained_pipeline, input_data)\n",
    "\n",
    "    return {\"model_score\": score, \"prediction\": prediction}\n"
   ],
   "id": "35611082b4270aa1"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
